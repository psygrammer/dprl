{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 의사결정RL : 파트 5 - 딥강화학습 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* Abstract\n",
    "* 1 INTRODUCTION\n",
    "* 2 PRELIMINARIES\n",
    "    - 2.1 Supervised Approach to Imitation\n",
    "    - 2.2 Forward Training\n",
    "    - 2.3 Stochastic Mixing Iterative Learning\n",
    "* 3 DATASET AGGREGATION\n",
    "* 4 THEORETICAL ANALYSIS\n",
    "    - 4.1 Online Learning\n",
    "    - 4.2 No Regret Algorithms Guarantees\n",
    "* 5 EXPERIMENTS\n",
    "    - 5.1 Super Tux Kart\n",
    "    - 5.2 Super Mario Bros\n",
    "    - 5.3 Handwriting Recognition\n",
    "* 6 FUTURE WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [2] (slide) Reduction of Imitation Learning to No-Regret Online Learning - https://www.cs.cmu.edu/~sross1/publications/Ross-AIStats11-Slides.pdf\n",
    "* [3] (python code) Dagger : Dataset Aggregation for Learning to Search - https://github.com/Refefer/Dagger\n",
    "* [4] A dagger by any other name: scheduled sampling - http://nlpers.blogspot.kr/2016/03/a-dagger-by-any-other-name-scheduled.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [5] Regret Minimization: Algorithms and Applications - http://slideplayer.com/slide/4108414/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting.\n",
    "* Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learn- ing. \n",
    "* This leads to poor performance in theory and often in practice. Some recent approaches (Daumé III et al., 2009; Ross and Bagnell, 2010) provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imitation learning\n",
    "* poor performance\n",
    "* Recent approaches\n",
    "    - (Ross and Bagnell, 2010) \n",
    "    - SMILe (Ross and Bagnell, 2010)\n",
    "\n",
    "We propose a new meta-algorithm for imitation learning which \n",
    "* learns a stationary deterministic policy guaranteed to perform well \n",
    "    - under its induced distribution of states \n",
    "    - (number of mistakes/costs that \n",
    "        - grows linearly in T and \n",
    "        - classification cost ε).\n",
    "* Our approach is closely related to no regret online learning algorithms\n",
    "\n",
    "We begin by establishing our notation and setting, discuss related work, and then present the DAGGER (Dataset Ag- gregation) method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 PRELIMINARIES\n",
    "* 2.1 Supervised Approach to Imitation\n",
    "* 2.2 Forward Training\n",
    "* 2.3 Stochastic Mixing Iterative Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find a policy $πˆ$ which minimizes the observed surrogate loss under its induced distribution of states, i.e.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote $l$ the observed surrogate loss function we minimize instead of $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Supervised Approach to Imitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The traditional approach to imitation learning ignores the change in distribution and simply trains a policy π that performs well under the distribution of states encountered by the expert dπ∗ . \n",
    "* This can be achieved using any standard supervised learning algorithm. It finds the policy πˆsup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this bound is tight, i.e. there exist problems such that a policy π with ε 0-1 loss on dπ∗ can incur ex- tra cost that grows quadratically in T ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Forward Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward training algorithm introduced by Ross and Bagnell (2010) trains a non-stationary policy (one policy πt for each time step t) iteratively over T iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### worst case "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the worst case, u could be O(T) and the forward algorithm wouldn’t provide any improvement over the traditional supervised learning approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in many cases u is O(1) or sub-linear in T and the forward algorithm leads to improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Stochastic Mixing Iterative Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMILe, proposed by Ross and Bagnell (2010), alleviates\n",
    "this problem and can be applied in practice when T is\n",
    "large or undefined by adopting an approach similar to\n",
    "SEARN (Daumé III et al., 2009) where a stochastic stationary policy is trained over several iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 DATASET AGGREGATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now present DAGGER (Dataset Aggregation), an iterative algorithm that trains a deterministic policy that achieves good performance guarantees under its induced distribution of states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAGGER proceeds by collecting a dataset at each iteration under the current policy and trains the next policy under the aggregate of all collected datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finite Sample Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 THEORETICAL ANALYSIS\n",
    "* 4.1 Online Learning\n",
    "* 4.2 No Regret Algorithms Guarantees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theoretical analysis of DAGGER only relies on the no- regret property of the underlying Follow-The-Leader algorithm on strongly convex losses (Kakade and Tewari, 2009) which picks the sequence of policies πˆ1:N ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Online Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In online learning, an algorithm must provide a policy $π_n$ at iteration n which incurs a loss $l_n(π_n)$. \n",
    "\n",
    "After observing this loss, the algorithm can provide a different policy $π_{n+1}$ for the next iteration which will incur loss $l_{n+1}(π_{n+1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A no-regret algorithm is an algo- rithm that produces a sequence of policies $π_1 , π_2 , . . . , π_N$ such that the average regret with respect to the best policy in hindsight goes to 0 as N goes to ∞:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap9.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 No Regret Algorithms Guarantees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finite Sample Case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous results hold if the on- line learning algorithm observes the infinite sample loss, i.e. the loss on the true distribution of trajectories induced by the current policy $π_i$. \n",
    "\n",
    "In practice however the algorithm would only observe its loss on a small sample of trajecto- ries at each iteration. We wish to bound the true loss under its own distribution of the best policy in the sequence as a function of the regret on the finite sample of trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap14.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 EXPERIMENTS\n",
    "* 5.1 Super Tux Kart\n",
    "* 5.2 Super Mario Bros\n",
    "* 5.3 Handwriting Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Super Tux Kart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap15.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap16.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Super Mario Bros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap17.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap18.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Handwriting Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we demonstrate the efficacy of our approach on a structured prediction problem involving recognizing hand- written words given the sequence of images of each character in the word. We follow Daumé III et al. (2009) in adopt- ing a view of structured prediction as a degenerate form of imitation learning where the system dynamics are deter- ministic and trivial in simply passing on earlier predictions made as inputs for future predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider predicting the word by predicting each character in sequence in a left to right order, using the previously predicted character to help predict the next and a linear SVM, following the greedy SEARN approach in Daumé III et al. (2009)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also compare these ap- proaches to two baseline, a non-structured approach which simply predicts each character independently and the supervised training approach where training is conducted with the previous character always correctly labeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap19.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 FUTURE WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1] A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning - http://arxiv.org/pdf/1011.0686v3.pdf\n",
    "* [2] (slide) Reduction of Imitation Learning\n",
    "to No-Regret Online Learning -  https://www.cs.cmu.edu/~sross1/publications/Ross-AIStats11-Slides.pdf\n",
    "* [3] (python code) Dagger : Dataset Aggregation for Learning to Search - https://github.com/Refefer/Dagger\n",
    "* [4] A dagger by any other name: scheduled sampling - http://nlpers.blogspot.kr/2016/03/a-dagger-by-any-other-name-scheduled.html\n",
    "* [5] Regret Minimization: Algorithms and Applications - http://slideplayer.com/slide/4108414/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
