{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 싸이그래머 / 의사결정RL : 파트 6 - 강화학습 대화형 에이전트 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* 1.1 The Design Problem for Spoken Dialogue Systems\n",
    "* 1.2 Overview \n",
    "* 1.3 Structure of the Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "\n",
    "##### WOZ\n",
    "* [2] Wizard of Oz experiment (Wikipedia) - https://en.wikipedia.org/wiki/Wizard_of_Oz_experiment\n",
    "* [3] Language Technology II Language-Based Interaction: Dialogue design and evaluation - http://www.coli.uni-saarland.de/courses/late2/slides/NLI%20-%20Design%20and%20evaluation_Web.ppt\n",
    "\n",
    "##### 최신 동향\n",
    "* [4] Chat Bots - http://web.stanford.edu/class/cs124/lec/chatbot.pdf\n",
    "* [5] Deep Reinforcement Learning for Dialogue Generation - https://arxiv.org/abs/1606.01541"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The past decade has seen something of a revolution in the field of spoken dialogue systems. \n",
    "* As in other areas of Computer Science and Artificial Intelligence, data-driven methods are being used to drive new methodologies for system development and evaluation. \n",
    "* These methods are proving to be more robust, flexible, and adaptive than the rule-based approaches which preceded them.\n",
    "* This book is therefore intended as a guide which navigates through a detailed case study in data-driven methods for development and evaluation of spoken dialogue systems. \n",
    "* It focusses on Dialogue Management and Natural Language Gener- ation, rather than speech recognition and spoken language understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 The Design Problem for Spoken Dialogue Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dialogue strategies\n",
    "\n",
    "* The design of Spoken Dialogue Systems (SDS) is not only concerned with integrating speech and language processing modules such as \n",
    "    - Automatic Speech Recognition (ASR), \n",
    "    - Spoken Language Understanding (SLU), \n",
    "    - Natural Language Generation (NLG), and \n",
    "    - Text-to-Speech (TTS) synthesis systems.\n",
    "* It also requires the development of skills for “what to say next”:\n",
    "    - dialogue strategies \n",
    "        - which take into account the performance of these components, \n",
    "            - the nature of the user’s tasks \n",
    "                - (e.g.information-seeking, \n",
    "                - tutoring, or \n",
    "                - robot control), and \n",
    "            - other features of the operating environment such as \n",
    "                - the user’s behaviour and \n",
    "                - preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  rule-based, hand-coded strategies\n",
    "\n",
    "* In conventional, rule-based, dialogue development many expensive iterations of manual design and re-design are necessary in order to produce good strategies. \n",
    "* In addition, such hand-coded strategies are not re-usable from task to task, are not scalable, require a substantial amount of human labour and expertise, and are not guaranteed to be optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### machine learning methods\n",
    "\n",
    "For these reasons machine learning methods (such as Reinforcement Learning) for dialogue strategy design have been a leading research area for several years.\n",
    "* a data-driven automatic development cycle\n",
    "* provably optimal action policies\n",
    "* a principled mathematical model for action selection\n",
    "* possibilities for generalisation to unseen states\n",
    "* reduced development and deployment costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chicken-and-egg\n",
    "\n",
    "* However, in cases where a system is designed from scratch, there is often no suitable in-domain data to enable such a design. \n",
    "* Collecting dialogue data without a working prototype is problematic, leaving the developer with a classic “chicken-and-egg” problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">One of the main issues that this book addresses is how to use a data-driven development methodology when little or no in-domain data exists.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simulation-based Reinforcement Learning\n",
    "\n",
    "In this book we propose to learn dialogue strategies by simulation-based Reinforcement Learning (RL) (Sutton and Barto, 1998), \n",
    "* where a simulated environment is learned \n",
    "    - from small amounts of Wizard-of-Oz (WOZ) data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wizard-of-Oz (WOZ) data\n",
    "\n",
    "* Using WOZ data rather than data from real Human-Computer Interaction (HCI) allows us to learn optimal strategies for domains where no working dialogue system already exists. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bootstrapping\n",
    "\n",
    "* Automatic strategy learning has been applied to dialogue systems which have already been deployed in the real world using handcrafted strategies\n",
    "    - In such work, strategy learning was performed based on already present extensive online-operation experience, e.g. (Henderson et al, 2005, 2008; Singh et al, 2002).\n",
    "* In contrast to this preceding work, our approach enables strategy learning in domains where no prior system is available. \n",
    "    - Optimised learned strategies are then available from the first moment of online-operation, and labour-intensive handcrafting of dialogue strategies is avoided. \n",
    "* <font color=\"red\">This independence from large amounts of in-domain dialogue data allows researchers to apply RL to new application areas beyond the scope of existing dialogue systems. We call this method “bootstrapping”.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-step procedure\n",
    "\n",
    "* This book first provides the general proof-of-concept that RL-based strategies outperform handcrafted strategies which are manually tuned for a wide spectrum of application scenarios. \n",
    "* We propose to learn dialogue strategies by simulation-based RL, where the simulated environment is learned from small amounts of WOZ data.\n",
    "* We therefore introduce a 5-step procedure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Collect data \n",
    "    - in a WOZ experiment.\n",
    "2. Use this data to construct a simulated learning environment \n",
    "    - using data-driven methods only.\n",
    "3. Traina RL-based dialogue policy \n",
    "    - by interacting with the simulated environment.\n",
    "    - We compare this policy \n",
    "        - against a supervised baseline. \n",
    "    - This comparison allows us to \n",
    "        - measure the relative improvements \n",
    "            - over the WOZ strategies contained in the training data.\n",
    "4. Evaluate the learned policy \n",
    "    - with real users.\n",
    "5. Show that “bootstrapping” from WOZ data \n",
    "    - is a valid estimate of real HCI \n",
    "        - by comparing different aspects of \n",
    "            - the 3 corpora gathered so far: \n",
    "                - the WOZ study, \n",
    "                - the dialogues generated in simulation, and \n",
    "                - the final user tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">We apply this framework to optimise multimodal Dialogue Management strategies and Natural Language Generation.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multimodal Dialogue Management strategies\n",
    "* In the first case we consider Dialogue Mangement and content selection as two closely interrelated problems for information seeking dialogues:\n",
    "    - the decision of <font color=\"red\">when to present information</font> \n",
    "    - depends on <font color=\"red\">how many pieces of information to present</font> \n",
    "        - and the available options for how to present them, and vice versa.\n",
    "* We therefore formulate the problem as a <font color=\"blue\">hierarchy of joint learning decisions</font> which are optimised together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Language Generation (NLG)\n",
    "* The second study describes a new approach to generating Natural Language in interactive systems. \n",
    "* Natural Language Generation (NLG) addresses the problem of \n",
    "    - <font color=\"red\">“how to say”</font> an utterance, \n",
    "        - once “what to say” has been determined by the Dialogue Manager.\n",
    "* We treat NLG as planning under uncertainty \n",
    "    - for information-seeking dialogue systems, \n",
    "    - where the strategy for \n",
    "        - information presentation and\n",
    "        - its associated attributes \n",
    "        - are incrementally selected \n",
    "            - using hierarchical learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL vs SL\n",
    "* Our results in both studies show that RL significantly outperforms supervised learning (SL) when interacting in simulation as well as for interactions with real users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### objective funtion\n",
    "\n",
    "* One focus of this book is to optimise dialogue strategies with respect to real user preferences. A major advantage of RL-based dialogue strategy development is that the dialogue strategy can be automatically trained and evaluated using the same objective function \n",
    "* This book is the first to explore learning with data-driven, non-linear objective functions. \n",
    "* We also propose a new method for meta-evaluation of the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Structure of the Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Chapter 2 (Background)\n",
    "* This chapter provides the reader with relevant background knowledge for the re- search. After introducing some general information about Spoken Dialogue Sys- tems, we contrast different methods applied in research and industry to develop dialogue strategies.\n",
    "\n",
    "#### Chapter 3 (Reinforcement Learning)\n",
    "* This chapter provides technical background on RL for dialogue strategy develop- ment and discusses simulation-based learning in particular.\n",
    "\n",
    "#### Chapter 4 (Proof-of-Concept: Information Seeking Strategies)\n",
    "* He we develop the theoretical proof-of-concept that RL-based strategies outperform hand-coded strategies, which are tuned to the same objective function.\n",
    "* We show this for a wide range of application scenarios, e.g. for different user types and noise conditions. \n",
    "* This chapter also demonstrates how to apply simulation-based RL to solve a complex and challenging problem for information-seeking dialogue systems\n",
    "\n",
    "#### Chapter 5 (A Bootstrapping Approach to Develop Reinforcement Learning-based Strategies)\n",
    "* This chapter introduces a 5-step procedure model to bootstrap optimal RL-based strategies for WOZ data.\n",
    "\n",
    "#### Chapter 6 (Data Collection in a Wizard-of-Oz Experiment)\n",
    "* Here we describe the experimental setup of the WOZ experiment. \n",
    "* We explain which changes to the conventional WOZ method are necessary for strategy learning.\n",
    "\n",
    "#### Chapter 7 (Building a Simulated Learning Environment from Wizard-of-Oz Data)\n",
    "* This chapter uses the WOZ data to construct a simulated learning environment. \n",
    "* We therefore introduce methods suited to build and validate simulations from small amounts of data.\n",
    "\n",
    "#### Chapter 8 (Comparing Reinforcement and Supervised Learning of Dialogue Policies with Real Users)\n",
    "* In this chapter we evaluate the learned strategy with real users. \n",
    "* We therefore develop a music-player dialogue system using a rapid development tool, where the learned strategy is implemented using a table look-up between states and learned actions. \n",
    "* We report detailed results from the real user tests.\n",
    "\n",
    "#### Chapter 9 (Natural Language Generation)\n",
    "* This chapter further develops the methodology to encompass elements of policy learning for adaptive Natural Language Generation in spoken dialogue systems.\n",
    "\n",
    "#### Chapter 10 (Conclusion)\n",
    "* Finally, we conclude by summarising the main contributions of this work. * We also report on “lessons learned” to provide guidance for future researchers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [1] Reinforcement Learning for Adaptive Dialogue Systems: A Data-driven Methodology for Dialogue Management and Natural Language Generation - https://www.amazon.com/Reinforcement-Learning-Adaptive-Dialogue-Systems/dp/3642439845\n",
    "* [2] Wizard of Oz experiment (Wikipedia) - https://en.wikipedia.org/wiki/Wizard_of_Oz_experiment\n",
    "* [3] Language Technology II Language-Based Interaction: Dialogue design and evaluation - http://www.coli.uni-saarland.de/courses/late2/slides/NLI%20-%20Design%20and%20evaluation_Web.ppt\n",
    "* [4] Chat Bots - http://web.stanford.edu/class/cs124/lec/chatbot.pdf\n",
    "* [5] Deep Reinforcement Learning for Dialogue Generation - https://arxiv.org/abs/1606.01541"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
