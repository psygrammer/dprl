{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with Unsupervised Auxiliary Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 싸이그래머 / DGM : 파트 1 - 딥마인드 논문 리뷰 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* 1 RELATED WORK\n",
    "* 2 BACKGROUND\n",
    "* 3 AUXILIARY TASKS FOR REINFORCEMENT LEARNING\n",
    "    - 3.1 AUXILIARY CONTROL TASKS\n",
    "    - 3.2 AUXILIARY REWARD TASKS\n",
    "    - 3.3 EXPERIENCE REPLAY\n",
    "    - 3.4 UNREAL AGENT\n",
    "* 4 EXPERIMENTS\n",
    "    - 4.1 LABYRINTH RESULTS\n",
    "    - 4.2 ATARI\n",
    "* 5 CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "* Deep reinforcement learning agents have achieved state-of-the-art results by \n",
    "    - <font color=\"red\">directly maximising cumulative reward</font>. \n",
    "* However, <font color=\"blue\">environments contain</font> \n",
    "    - a much wider variety of \n",
    "        - <font color=\"red\">possible training signals</font>. \n",
    "* In this paper, we introduce \n",
    "    - an agent that also maximises \n",
    "        - <font color=\"red\">many other pseudo-reward</font> functions \n",
    "            - <font color=\"blue\">simultaneously</font> by reinforcement learning. \n",
    "        - All of these tasks \n",
    "            - <font color=\"red\">share a common representation</font> that, like \n",
    "                - <font color=\"blue\">unsupervised learning</font>, \n",
    "                - continues to develop in \n",
    "                    - the <font color=\"blue\">absence of extrinsic rewards</font>. \n",
    "* We also introduce \n",
    "    - a novel mechanism for \n",
    "        - <font color=\"red\">focusing this representation upon extrinsic rewards</font>, \n",
    "        - so that learning <font color=\"red\">can rapidly adapt</font> \n",
    "            - to the most <font color=\"blue\">relevant aspects of the actual task</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [2] (Vidoe) DeepMind - Reinforcement Learning with Unsupervised Auxiliary Tasks - https://youtu.be/Uz-zGYrYEjA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/deepmind-live-cms/documents/iclrgif.gif\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stream of sensorimotor data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Natural and artificial agents <font color=\"red\">live in a stream of sensorimotor data</font>. \n",
    "* These actions <font color=\"red\">influence the future course of the sensorimotor stream</font>. \n",
    "* In this paper we develop agents that <font color=\"red\">learn to predict and control this stream</font>, by solving a host of reinforcement learning problems, <font color=\"red\">each focusing on a distinct feature</font> of <font color=\"blue\">the sensorimotor stream</font>. \n",
    "*  <font color=\"red\">Our hypothesis</font> is that an agent that  <font color=\"red\">can flexibly control its future experiences</font> will also be able to  <font color=\"red\">achieve any goal with which it is presented</font>, such as maximising its future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pseudo-rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The classic reinforcement learning paradigm focuses on the maximisation of extrinsic reward. \n",
    "* However, in many interesting domains, <font color=\"red\">extrinsic rewards are only rarely observed</font>.\n",
    "* Even if extrinsic rewards are frequent, the sensorimotor stream <font color=\"red\">contains an abundance of other possible learning targets</font>.\n",
    "* <font color=\"blue\">Traditionally, unsupervised learning</font> attempts to <font color=\"blue\">reconstruct these targets</font>, such as the pixels in the current or subsequent frame. It is typically <font color=\"blue\">used to accelerate the acquisition of a useful representation</font>.\n",
    "* In contrast, our learning objective is to predict and control features of the sensorimotor stream, by treating them as pseudo-rewards for reinforcement learning. Intuitively, this set of tasks is more closely matched with the agent’s long-term goals, potentially leading to more useful representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### experience replay mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 참고 \n",
    "* [4] Experience Replay - http://unpredictablepattern.blogspot.kr/2016/02/experience-replay.html\n",
    "* [5] The future of memory: remembering, imagining, and the brain - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3815616/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our architecture uses reinforcement learning to approximate both the optimal policy and optimal value function for many different pseudo-rewards.\n",
    "* These include the long-term goal of predicting cumulative extrinsic reward as well as short-term predictions of extrinsic reward.\n",
    "* To learn more efficiently, <font color=\"red\">our agents use an experience replay mechanism</font> to provide additional updates to the critics.\n",
    "    - Just as animals dream about positively or negatively rewarding events more frequently (Schacter et al., 2012), our agents preferentially replay sequences containing rewarding events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### jointly learned representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Importantly, both the auxiliary control and auxiliary prediction tasks\n",
    "    - share \n",
    "        - the convolutional neural network and \n",
    "        - LSTM \n",
    "    - that the base agent uses to act. \n",
    "* By using this jointly learned representation, \n",
    "    - the base agent learns to optimise\n",
    "        - extrinsic reward much faster and, \n",
    "        - in many cases, \n",
    "            - achieves better policies at the end of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asynchronous Advantage Actor-Critic (A3C) framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 참고\n",
    "* [6] 텐서플로우 설치도 했고 튜토리얼도 봤고 기초 예제도 짜봤다면 TensorFlow KR Meetup 2016 - http://www.slideshare.net/carpedm20/ss-63116251"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper brings together the state-of-the-art Asynchronous Advantage Actor-Critic (A3C) framework (Mnih et al., 2016), outlined in Section 2,\n",
    "* with \n",
    "    - auxiliary control tasks and \n",
    "    - auxiliary reward tasks, \n",
    "        - defined in sections Section 3.1 and Section 3.2 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### auxiliary tasks\n",
    "* These auxiliary tasks \n",
    "    - do not require \n",
    "        - any extra supervision or \n",
    "        - signals from the environment \n",
    "            - than the vanilla A3C agent. \n",
    "* The result is our UNsupervised REinforcement and Auxiliary Learning (UNREAL) agent (Section 3.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UREAL agent\n",
    "* In Section 4 we apply our UNREAL agent \n",
    "    - to a challenging set of \n",
    "        - 3D-vision based domains known as the Labyrinth (Mnih et al., 2016), \n",
    "            - learning solely from the raw RGB pixels of a first-person view. \n",
    "* Our agent significantly outperforms the baseline agent using vanilla A3C, even when the baseline was augmented with an unsupervised reconstruction loss, in terms of speed of learning, robustness to hyperparameters, and final performance. \n",
    "* The result is an agent \n",
    "    - which on average achieves 87% of expert human-normalised score,\n",
    "    - compared to 54% with A3C, \n",
    "    - and on average 10× faster than A3C. \n",
    "* Our UNREAL agent also significantly outperforms \n",
    "    - the previous state-of-the-art in the Atari domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 RELATED WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A variety of reinforcement learning architectures have focused on learning temporal abstractions, such as options (Sutton et al., 1999b), with policies that may maximise pseudo-rewards (Konidaris & Barreto, 2009; Silver & Ciosek, 2012).\n",
    "    - The emphasis here has typically been on the development of temporal abstractions that facilitate high-level learning and planning.\n",
    "    - In contrast, our agents do not make any direct use of the pseudo-reward maximising policies that they learn (although this is an interesting direction for future research). \n",
    "    - Instead, they are used solely as auxiliary objectives for developing a more effective representation.\n",
    "* The Horde architecture (Sutton et al., 2011) also applied reinforcement learning to identify value functions for a multitude of distinct pseudo-rewards.\n",
    "    - However, this architecture was not used for representation learning; instead each value function was trained separately using distinct weights.\n",
    "* The UVFA architecture (Schaul et al., 2015a) is a factored representation of a continuous set of optimal value functions, combining features of the state with an embedding of the pseudo-reward function. \n",
    "* Similarly, the successor representation (Dayan, 1993; Barreto et al., 2016; Kulkarni et al., 2016) factors a continuous set of expected value functions for a fixed policy, by combining an expectation over features of the state with an embedding of the pseudo-reward function.\n",
    "* Another, related line of work involves learning models of the environment (Schmidhuber, 2010; Xie et al., 2015; Oh et al., 2015). Although learning environment models as auxiliary tasks could improve RL agents (e.g. Lin & Mitchell (1992); Li et al. (2015)), this has not yet been shown to work in rich visual environments.\n",
    "* More recently, auxiliary predictions tasks have been studied in 3D reinforcement learning environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 BACKGROUND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 AUXILIARY TASKS FOR REINFORCEMENT LEARNING\n",
    "* 3.1 AUXILIARY CONTROL TASKS\n",
    "* 3.2 AUXILIARY REWARD TASKS\n",
    "* 3.3 EXPERIENCE REPLAY\n",
    "* 3.4 UNREAL AGENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we incorporate auxiliary tasks into the reinforcement learning framework in order to promote faster training, more robust learning, and ultimately higher performance for our agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 AUXILIARY CONTROL TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### auxiliary control tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The auxiliary control tasks \n",
    "    - we consider are defined \n",
    "        - as additional pseudo-reward functions \n",
    "            - in the environment the agent is interacting with. \n",
    "* We formally define \n",
    "    - an auxiliary control task c \n",
    "        - by a reward function $r^{(c)}$ : S × A → R, \n",
    "            - where \n",
    "                - S is the space of possible states and \n",
    "                - A is the space of available actions.\n",
    "* The underlying state space S \n",
    "    - includes both \n",
    "        - the history of observations and \n",
    "        - rewards as well as \n",
    "            - the state of the agent itself, \n",
    "                - i.e. the activations of the hidden units of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given a set of auxiliary control tasks C, \n",
    "    - let $π^{(c)}$ be the agent’s policy \n",
    "        - for each auxiliary task c ∈ C and \n",
    "        - let π be the agent’s policy on the base task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ovrall objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall objective is to maximise total performance across all these auxiliary tasks,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss fuction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In principle, any reinforcement learning method could be applied to maximise these objectives. \n",
    "* However, to efficiently learn to maximise many different pseudo-rewards simultaneously in parallel from a single stream of experience, it is necessary to use off-policy reinforcement learning. \n",
    "* We focus on value-based RL methods that approximate the optimal action-values by Q-learning. \n",
    "* Specifically, for each control task c we optimise an \n",
    "    - n-step Q-learning loss \n",
    "    \n",
    "    $L^{(c)}_Q$ = E[($R_{t:t+n}$ + $γ^n$$max_{a′}$$Q^{(c)}$(s′,a′,$θ^-$)− $Q^{(c)}$(s,a,θ)$)^2$]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### auxiliary reward functions\n",
    "* Pixel changes \n",
    "    - Changes in the perceptual stream often correspond to important events in an environment. \n",
    "    - We train agents that \n",
    "        - learn a separate policy \n",
    "            - for maximally changing the pixels \n",
    "                - in each cell \n",
    "                    - of an n × n non-overlapping grid \n",
    "                        - placed over the input image. \n",
    "    - We refer to these auxiliary tasks as <font color=\"red\">pixel control</font>. \n",
    "* Network features \n",
    "    - Since the policy or value networks of an agent learn to extract task-relevant high-level features of the environment (Mnih et al., 2015; Zahavy et al., 2016; Silver et al., 2016) \n",
    "        - they can be useful quantities for the agent to learn to control. \n",
    "    - Hence, the activation of <font color=\"blue\">any hidden unit of the agent’s neural network can itself be an auxiliary reward</font>. \n",
    "    - We train agents that \n",
    "        - learn a separate policy \n",
    "            - for maximally activating \n",
    "                - each of the units \n",
    "                    - in a specific hidden layer. \n",
    "    - We refer to these tasks as <font color=\"red\">feature control</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Figure 1 (b) shows \n",
    "    - an A3C agent architecture \n",
    "        - augmented with \n",
    "            - a set of auxiliary pixel control tasks. \n",
    "     - In this case, \n",
    "         - the base policy π shares \n",
    "             - both \n",
    "                 - the convolutional visual stream and \n",
    "                 - the LSTM with the auxiliary policies.\n",
    "* The output of the auxiliary network head is \n",
    "    - an $N_{act}$ × n × n tensor $Q^{aux}$ \n",
    "        - where $Q^{aux}$(a,i,j) \n",
    "            - represents the network’s current estimate of \n",
    "                - the optimal discounted expected change \n",
    "                - in cell (i, j) of the input after taking action a. \n",
    "* We exploit \n",
    "    - the spatial nature of \n",
    "        - the auxiliary tasks \n",
    "            - by using a deconvolutional neural network \n",
    "    - to produce the auxiliary values $Q^{aux}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 AUXILIARY REWARD TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In addition to learning generally about the dynamics of the environment, an agent must learn to maximise the global reward stream. \n",
    "* To learn a policy to maximise rewards, an agent requires features that recognise states that lead to high reward and value. \n",
    "* <font color=\"red\">An agent with a good representation of rewarding states, will allow the learning of good value functions, and in turn should allow the easy learning of a policy</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=\"red\">However, in many interesting environments reward is encountered very sparsely</font>, meaning that it can take a long time to train feature extractors adept at recognising states which signify the onset of reward. \n",
    "* <font color=\"red\">We want to remove the perceptual sparsity of rewards</font> and rewarding states to aid the training of an agent, but to do so in a way which does not introduce bias to the agent’s policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reward prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To do this, we introduce the auxiliary task of reward prediction \n",
    "    - that of predicting the onset of immediate reward given some historical context. \n",
    "    - This task consists of processing a sequence of consecutive observations, and requiring <font color=\"red\">the agent to predict the reward picked up in the subsequent unseen frame</font>. \n",
    "    - This is similar to value learning focused on immediate reward (γ = 0).\n",
    "* We train the reward prediction task on sequences \n",
    "    - $S_τ$ = ($s_{τ−k}$,$s_{τ−k+1}$,...,$s_{τ−1}$) \n",
    "        - to predict the reward $r_τ$ , and \n",
    "        - sample $S_τ$ from the experience of our policy π \n",
    "            - in a skewed manner \n",
    "    - so as to overrepresent rewarding events \n",
    "        - (presuming rewards are sparse within the environment).\n",
    "* Specifically, \n",
    "    - we sample such that\n",
    "        - zero rewards and non-zero rewards \n",
    "            - are equally represented, \n",
    "            - i.e. the predicted probability of \n",
    "                - a non-zero reward is P($r_τ$=0)=0.5.\n",
    "* The reward prediction is trained to minimise a loss $L_RP$. \n",
    "* In our experiments \n",
    "    - we use a multiclass cross-entropy classification loss across \n",
    "        - three classes (zero, positive, or negative reward), \n",
    "            - although a mean-squared error loss is also feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The auxiliary reward predictions may use a different architecture to the agent’s main policy. \n",
    "* Rather than simply “hanging” the auxiliary predictions off the LSTM,\n",
    "    - we use a simpler feedforward network \n",
    "        - that concatenates as tack of states $S_τ$\n",
    "            - after being encoded by the agent’s CNN, see Figure 1 (c)\n",
    "    - The idea is to simplify \n",
    "        - the temporal aspects of \n",
    "            - the prediction task \n",
    "        - in both \n",
    "            - the future direction \n",
    "                - (focusing only on immediate reward prediction rather than long-term returns) and \n",
    "            - past direction \n",
    "                - (focusing only on immediate predecessor states rather than the complete history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 EXPERIENCE REPLAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Experience replay has proven to be an effective mechanism for improving both the data efficiency and stability of deep reinforcement learning algorithms (Mnih et al., 2015). \n",
    "* <font color=\"red\">The main idea is to store transitions in a replay buffer, and then apply learning updates to sampled transitions from this buffer</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skewed sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Experience replay provides a natural mechanism for skewing the distribution of reward prediction samples towards rewarding events: \n",
    "    - we simply split the replay buffer into \n",
    "        - rewarding and \n",
    "        - non-rewarding subsets, \n",
    "    - and replay equally from both subsets. \n",
    "* <font color=\"red\">The skewed sampling of transitions from a replay buffer means</font> \n",
    "    - that <font color=\"red\">rare rewarding states will be oversampled</font>, and \n",
    "    - learnt from far more frequently than \n",
    "        - if we sampled sequences directly from the behaviour policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### value function replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In addition to reward prediction, we also use the replay buffer to perform value function replay. \n",
    "* This amounts to \n",
    "    - resampling recent historical sequences \n",
    "        - from the behaviour policy distribution and \n",
    "    - performing extra value function regression \n",
    "        - in addition to the on-policy value function regression in A3C. \n",
    "* By resampling \n",
    "    - previous experience, and \n",
    "    - randomly varying the temporal position \n",
    "        - of the truncation window over \n",
    "            - which the n-step return is computed, \n",
    "    - <font color=\"red\">value function replay performs</font> \n",
    "        - <font color=\"blue\">value iteration</font> and \n",
    "        - <font color=\"blue\">exploits newly discovered features</font> \n",
    "            - shaped by reward prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Experience replay is also used to increase the efficiency and stability of the auxiliary control tasks. Q-learning updates are applied to sampled experiences that are drawn from the replay buffer, allowing features to be developed extremely efficiently.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 UNREAL AGENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UNREAL algorithm combines the benefits of two separate, state-of-the-art approaches to deep reinforcement learning. \n",
    "* The primary policy is trained with A3C (Mnih et al., 2016): \n",
    "    - it learns from parallel streams of experience to gain efficiency and stability; \n",
    "    - it is updated online using policy gradient methods; \n",
    "    - and it uses a recurrent neural network \n",
    "        - to encode the complete history of experience. \n",
    "* This allows the agent to learn effectively in partially observed environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The auxiliary tasks are trained on very recent sequences of experience that are stored and randomly sampled; \n",
    "* these sequences may be prioritised (in our case according to immediate rewards) (Schaul et al., 2015b); \n",
    "* these targets are trained off-policy by Q-learning; \n",
    "* and they may use simpler feedforward architectures. \n",
    "* This allows the representation to be trained with maximum efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UNREAL algorithm optimises a single combined loss function with respect to the joint parameters of the agent, θ, \n",
    "* that combines \n",
    "    - the A3C loss $L_{A3C}$ together with \n",
    "    - an auxiliary control loss $L_{PC}$, \n",
    "    - auxiliary reward prediction loss $L_{RP}$ and \n",
    "    - replayed value loss $L_{VR}$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 EXPERIMENTS\n",
    "* 4.1 LABYRINTH RESULTS\n",
    "* 4.2 ATARI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 LABYRINTH RESULTS\n",
    "* 4.1.1 RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 ATARI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.png\" width=600 />\n",
    "<img src=\"figures/cap9.png\" width=600 />\n",
    "<img src=\"figures/cap10.png\" width=600 />\n",
    "<img src=\"figures/cap11.png\" width=600 />\n",
    "<img src=\"figures/cap12.png\" width=600 />\n",
    "<img src=\"figures/cap13.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [1] (paper) REINFORCEMENT LEARNING WITH UNSUPERVISED AUXILIARY TASKS - https://arxiv.org/pdf/1611.05397.pdf\n",
    "* [2] (Vidoe) DeepMind - Reinforcement Learning with Unsupervised Auxiliary Tasks - https://youtu.be/Uz-zGYrYEjA\n",
    "* [3] (blog) Reinforcement learning with unsupervised auxiliary tasks - https://deepmind.com/blog/reinforcement-learning-unsupervised-auxiliary-tasks/\n",
    "* [4] Experience Replay - http://unpredictablepattern.blogspot.kr/2016/02/experience-replay.html\n",
    "* [5] The future of memory: remembering, imagining, and the brain - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3815616/\n",
    "* [6] 텐서플로우 설치도 했고 튜토리얼도 봤고 기초 예제도 짜봤다면 TensorFlow KR Meetup 2016 - http://www.slideshare.net/carpedm20/ss-63116251"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
