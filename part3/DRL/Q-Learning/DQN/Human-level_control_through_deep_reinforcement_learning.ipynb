{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human-level control through deep reinforcement learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 싸이그래머 / 의사결정RL - 파트 4 : DeepRL [1, 2]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [3] Playing Atari With Deep Reinforcement Learning (NIPS 2013) 논문리뷰 - http://sanghyukchun.github.io/90/\n",
    "* [4] RL slide - https://computing.ece.vt.edu/~f15ece6504/slides/L26_RL.pdf\n",
    "* [12] Deep Reinforcement Learning - ICLR 2015 tutorial - http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf\n",
    "* [11] Deep Q-Learning - http://www.slideshare.net/nikolaypavlov/deep-qlearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://sanghyukchun.github.io/images/post/90-6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/rl-presentation-160522151115/95/deep-qlearning-9-1024.jpg?cb=1463930058\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/rl-presentation-160522151115/95/deep-qlearning-10-1024.jpg?cb=1463930058\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### action-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a deep convolutional neural network to approximate the optimal action-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/eqn.1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### experience replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform experience replay we store \n",
    "\n",
    "the agent’s experiences \n",
    "\n",
    "$e_t = (s_t,a_t,r_t,s_{t+1})$ at each time-step $t$ \n",
    "\n",
    "in a data set \n",
    "\n",
    "$D_t$ =  {$e_1$,...,$e_t$}. \n",
    "\n",
    "During learning, we apply Q-learning updates, \n",
    "\n",
    "on samples (or minibatches) of experience \n",
    "\n",
    "$(s,a,r,s')$ ~ $U(D)$, \n",
    "\n",
    "drawn uniformly at random from the pool of stored samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.1.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-learning update at iteration i uses the following loss function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/eqn.2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.2.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.10.png\" width=1000 />\n",
    "<img src=\"figures/capn.11.png\" width=600 />\n",
    "<img src=\"figures/capn.12.png\" width=600 />\n",
    "<img src=\"figures/capn.13.png\" width=1000 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.3.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.4.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [10] Distributed Deep Q-Learning - http://www.slideshare.net/onghaoyi/distributed-deep-qlearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/dist-deep-qlearn-slides-151023232840-lva1-app6891/95/distributed-deep-qlearning-13-1024.jpg?cb=1445643171\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 참고\n",
    "* [5] songrotek's code - https://github.com/songrotek/DQN-Atari-Tensorflow\n",
    "* [6] asrivat1's code - https://github.com/asrivat1/DeepLearningVideoGames\n",
    "* [7] gliese581gg's code - https://github.com/gliese581gg/DQN_tensorflow\n",
    "* [8] devsisters' code - https://github.com/devsisters/DQN-tensorflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source code can be accessed at https://sites.google.com/a/ deepmind.com/dqn for non-commercial uses only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.1.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/dist-deep-qlearn-slides-151023232840-lva1-app6891/95/distributed-deep-qlearning-14-638.jpg?cb=1445643171\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://sanghyukchun.github.io/images/post/90-5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.9.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/eqn.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads to a sequence of loss functions $L_i(\\theta_i)$ that changes at each iteration $i$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/eqn.4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiating the loss function with respect to the weights we arrive at the following gradient:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/eqn.5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training algorithm for deep Q-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/eqn.6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.5.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.6.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.7.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.8.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.14.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.15.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [1] Playing Atari With Deep Reinforcement Learning - http://arxiv.org/abs/1312.5602\n",
    "* [2] Human-level control through deep reinforcement learning - http://home.uchicago.edu/~arij/journalclub/papers/2015_Mnih_et_al.pdf\n",
    "* [3] Playing Atari With Deep Reinforcement Learning (NIPS 2013) 논문리뷰 -  http://sanghyukchun.github.io/90/\n",
    "* [4] RL slide - https://computing.ece.vt.edu/~f15ece6504/slides/L26_RL.pdf\n",
    "* [5] songrotek's code - https://github.com/songrotek/DQN-Atari-Tensorflow\n",
    "* [6] asrivat1's code - https://github.com/asrivat1/DeepLearningVideoGames\n",
    "* [7] gliese581gg's code - https://github.com/gliese581gg/DQN_tensorflow\n",
    "* [8] devsisters' code - https://github.com/devsisters/DQN-tensorflow/\n",
    "* [9] 강화학습 그리고 OpenAI - http://www.modulabs.co.kr/RL_library/3237\n",
    "* [10] Distributed Deep Q-Learning - http://www.slideshare.net/onghaoyi/distributed-deep-qlearning\n",
    "* [11] Deep Q-Learning - http://www.slideshare.net/nikolaypavlov/deep-qlearning\n",
    "* [12] Deep Reinforcement Learning - ICLR 2015 tutorial - http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
