
## 3.Review of Backpropagation and Numerical Optimization 
### ì˜ì‚¬ê²°ì •RL íŒŒíŠ¸3 ì´ìƒì—´

### Reference 
1. http://cs231n.stanford.edu/syllabus.html
2. http://ishuca.tistory.com/            


```python
from PIL import Image
im = Image.open('hnn.png')
import numpy as np
```


```python
im #Neural Network History
```




![png](3.Review_of_Backpropagation_files/3.Review_of_Backpropagation_2_0.png)



### Biological motivation and connections

- ë‡Œì˜ ê¸°ë³¸ ê³„ì‚°ì  ë‹¨ìœ„ ë‰´ëŸ°(neuron)
- ì•½ 10^14 â€“ 14^15ê°œì˜ ì‹œëƒ…ìŠ¤(synapses)
- ë‰´ëŸ°ì€ ìˆ˜ìƒëŒê¸°(dendrites)ìœ¼ë¡œë¶€í„° ì…ë ¥ ì‹ í˜¸ë¥¼ ë°›ê³  ì¶•ìƒ‰ ëŒê¸°(axon)ì— ì¶œë ¥ ì‹ í˜¸ë“¤ì„ ë§Œë“¦. ì‹œëƒ…ìŠ¤ ê°•ë„ì— ê¸°ë°˜í•˜ì—¬ ë‹¤ë¥¸ ë‰´ëŸ°ì˜ ìˆ˜ìƒëŒê¸°ì™€ ê³±í•´ì ¸ ìƒí˜¸ì‘ìš©í•¨.

<img src="http://cfile2.uf.tistory.com/image/256DA34256E1672C15B393">


```python
class Neuron(object):
  # ... 
  def forward(inputs):
    """ ì…ë ¥ë“¤ê³¼ ê°€ì¤‘ì¹˜ë“¤ì€ 1ì°¨ì›ì˜ numpy ë°°ì—´ì´ê³ , ë°”ì´ì–´ìŠ¤ëŠ” ìˆ«ìë¡œ ê°€ì •í•œë‹¤"""
    cell_body_sum = np.sum(inputs * self.weights) + self.bias
    firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # ì‹œê·¸ëª¨ì´ë“œ í™œì„± í•¨ìˆ˜
    return firing_rate
```

### í™œì„±í•¨ìˆ˜(Activation Function)
[1] ì‹œê·¸ëª¨ì´ë“œ(Sigmoid). 
- ì‹œê·¸ëª¨ì´ë“œ ë¹„ì„ í˜•ì„±ì€ ìˆ˜ë¦¬ì  í˜•íƒœ Ïƒ(x) = 1/ (1+ e^-x) (ë²”ìœ„ 0~1) ë§¤ìš°í° ìŒìˆ˜ëŠ” 0ì´ë˜ê³  ë§¤ìš°í° ì–‘ìˆ˜ëŠ” 1ì´ ë¨. ë‰´ëŸ°ì´ í™œì„±ë  ë•Œ 0ì´ë‚˜ 1ì˜ ê¼¬ë¦¬ì—ì„œ í¬í™”ë¨. ê·¸ë¼ì´ì–¸íŠ¸(ê¸°ìš¸ê¸°) ê±°ì˜ 0ì´ ë˜ë²„ë¦¼.vanishing gradient problem(ì§€ì—­ ìµœì†Œê°’ ë¨¸ë¬´ë¥´ê²Œ ë˜ëŠ” ì›ì¸) ì¶œë ¥ì´ 0ì´ ì¤‘ì‹¬ë˜ì§€ ì•ŠìŒ. ê·¸ë¼ì´ì–¸íŠ¸ëŠ” ì „ë¶€ ì–‘ì´ê±°ë‚˜ ì „ë¶€ ìŒì´ê±°ë‚˜

[2] ìŒê³¡íƒ„ì  íŠ¸(Tanh)
- ì‹¤ì œ ê°‘ì˜ ë²”ìœ„ë¥¼ -1ì—ì„œ 1ë¡œ ëŠ˜ë¦¼. ì¶œë ¥ì´ 0ì´ ì¤‘ì‹¬. ì‹œê·¸ëª¨ì´ë“œì™€ ê°™ì€ vanishing gradient problem

[3] ReLU(Rectified Linear Unit) f(x) = max(0,x)
- ì‹œê·¸ëª¨ì´ë“œ/ìŒê³¡íƒ„ì  íŠ¸ í•¨ìˆ˜ì™€ ë¹„êµí•´ì„œ ìˆ˜ë ´í•˜ëŠ”ë° ë¹ ë¥¸ ê°€ì†ë„ê°€ ìˆìŒ(ì„ í˜•ì„±) í¬í™”ë˜ì§€ ì•ŠìŒ. ì…ë ¥ê°’ì´ 0ë³´ë‹¤ ì‘ì„ ë•Œ ì¶œë ¥ë°§ë„ 0, ë¯¸ë¶„ê°’ë„ 0. ë•ë¶„ì— vanihing gradient problemì„ í•´ê²°í•  ìˆ˜ ìˆì§€ë§Œ í•„ìš”í•œ ì •ë³´ë¥¼ ìƒì„ìˆ˜ë„ ìˆìŒ. ReLUê°€ ì£½ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Leaky ReLUê³¼ ê°™ì€ ë°©ë²•ì´ ë‚˜ì™”ìŒ. 

<img src="http://cfile25.uf.tistory.com/image/212C443F56E172C81B6E69">
<img src="http://cfile29.uf.tistory.com/image/2529043F56E172CA1D6AF6">
<img src="http://cfile25.uf.tistory.com/image/251F493856E17D5132E2D3">

### ì‹ ê²½ë§ ì•„í‚¤í…ì³(Neural Network architectures)
- ê·¸ë˜í”„ì—ì„œ ë‰´ëŸ°ì€ ë¹„ìˆœí™˜ì ì¸ ê·¸ë˜í”„ì—ì„œ ë‰´ëŸ°ë“¤ì˜ ì§‘í•©. ì¼ë°˜ì ì¸ ë ˆì´ì–´ í˜•íƒœëŠ” ë‘ ì¸ì ‘ ë ˆì´ì–´ë“¤ ì‚¬ì´ì— ë‰´ëŸ°ë“¤ì˜ ì™„ì „íˆ ìŒëŒ€ ì—°ê²°ëœ ì™„ì „ì—°ê²° ë ˆì´ì–´(fully-connected layer)

<img src="http://cfile25.uf.tistory.com/image/2175203656E185AB035FEE">
<img src="http://cfile23.uf.tistory.com/image/226A103656E185AB0F5587">

###í”¼ë“œí¬ì›Œë“œ ê³„ì‚° ì˜ˆì œ(Example feed-forward computation)
####í™œì„± í•¨ìˆ˜ë¥¼ ê°–ëŠ” ë°˜ë³µì ì¸ í–‰ë ¬ ê³±ì…ˆ(Repeated matrix multiplications interwoven with activation function). 

### 3-ë ˆì´ì–´ ì‹ ê²½ë§ì˜ forward-pass:
```
f= lambda x: 1.0/(1.0 + np.exp(-x)) # í™œì„± í•¨ìˆ˜(ì‹œê·¸ëª¨ì´ë“œ ì‚¬ìš©)
x = np.random.randn(3, 1) # 3ê°œì˜ ìˆ«ìë¥¼ ê°–ëŠ” ë¬´ì‘ìœ„ ì…ë ¥ ë²¡í„°(3x1)
h1 = f(np.dot(W1, x) + b1) # ì²«ë²ˆì§¸ ì€ë‹‰ ì¸µì˜ í™œì„±í™”ë¥¼ ê³„ì‚° (4x1)
h2 = f(np.dot(W2, h1) + b2) # ë‘ë²ˆì§¸ ì€ë‹‰ ì¸µì˜ í™œì„±í™”ë¥¼ ê³„ì‚°(4x1)
out = np.dot(W3, h2) + b3 # ì¶œë ¥ ë‰´ëŸ° (1x1)
```

### ë ˆì´ì–´ì˜ ìˆ˜ì™€ í¬ê¸° ì„¤ì •í•˜ê¸°(Setting number of layers and their sizes)

ì˜ˆ) 2ì°¨ì›ì—ì„œ ì´í•­ë¶„ë¥˜ê¸° ë¬¸ì œê°€ ìˆë‹¤ê³  ê°€ì •. 3ê°œì˜ ë‹¤ë¥¸ ì‹ ê²½ë§ì„ í•™ìŠµí•  ìˆ˜ ìˆìŒ. ê°ê° í•˜ë‚˜ì˜ ì€ë‹‰ì¸µì˜ í¬ê¸° ë‹¤ë¦„.

<img src="http://cfile25.uf.tistory.com/image/27453C3656E28DAC0AEF2D">

ConvNetJS Demo : http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html

ê³¼ì í•©(overfitting) 
ëª¨ë¸ì´ ë³µì¡í• ìˆ˜ë¡ í•™ìŠµ ìë£Œì—ëŠ” ì í•©í•˜ì§€ë§Œ í…ŒìŠ¤íŠ¸ ì§‘í•©ì—ëŠ” ì¼ë°˜í™”ë˜ì§€ ì•ŠìŒ. 
ê³¼ì í•©ì„ í”¼í•˜ê¸° ìœ„í•´ L2 ì •ê·œí™”, dropout ì…ë ¥ ë…¸ì´ì¦ˆë¥¼ í†µí•´ í†µì œí•¨. dropout : í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ ë¬´ì‘ìœ„ë¡œ 50%ì˜ ì€ë‹‰ ë…¸ë“œë¥¼ ì œê±°í•œì±„ë¡œ í•™ìŠµì‹œí‚´, L2 ì •ê·œí™” : íŒŒë¼ë¯¸í„°ì— ì œí•œì„ ê±°ëŠ” ê²ƒ. ë²Œì í™”. 

<img src="http://cfile8.uf.tistory.com/image/242A0F4456E29224035DA3">

L1 regularization on least squares:
<img src="http://www.chioka.in/wp-content/uploads/2013/12/least_squares_l11.png"> 

- ê° ê°€ì¤‘ì¹˜ wëŠ” ëª©ì í•¨ìˆ˜ì— Î»|w|ë¡œ ë”í•´ì§„ë‹¤
- L1 ì •ê·œí™”ì™€ L2 ì •ê·œí™”ë¥¼ ì„ì–´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. Î»1|w| + Î»2w^2 (ì´ê²ƒì„ Elastic net regularizationì´ë¼ ë¶€ë¥¸ë‹¤)
- L1 ì •ê·œí™”ëŠ” ìµœì í™” ë™ì•ˆ ê°€ì¤‘ì¹˜ ë²¡í„°ë“¤ì´ í¬ì†Œí•´ì§€ëŠ” ê²ƒì„ ì´ë„ëŠ” í¥ë¯¸ë¡œìš´ íŠ¹ì„±ì´ ìˆë‹¤ (i.e. ì •í™•íˆ 0ì— ë§¤ìš° ê°€ê¹Œì›Œì§„ë‹¤). ì¦‰, L1 ì •ê·œí™”ë¥¼ ê°€ì§„ ë‰´ëŸ°ë“¤ì€ ê²°êµ­ ê·¸ë“¤ì˜ ê°€ì¥ ì¤‘ìš”í•œ ì…ë ¥ë³€ìˆ˜ë“¤ì˜ í¬ì†Œ ë¶€ë¶„ì§‘í•©ì„ ì‚¬ìš©í•˜ê³ , "noisy" ì…ë ¥ë³€ìˆ˜ë“¤ì— ê±°ì˜ ë³€í•˜ì§€ ì•Šê²Œ ëœë‹¤.

L2 regularization on least squares:
<img src="http://www.chioka.in/wp-content/uploads/2013/12/least_squares_l2.png">

- ë„¤íŠ¸ì›Œí¬ì—ì„œ ëª¨ë“  ê°€ì¤‘ì¹˜ì— ëŒ€í•´, ìš°ë¦¬ëŠ” 1/2 Î» w^2 ë¥¼ ëª©ì í•¨ìˆ˜ì— ë”í•œë‹¤(Î»ëŠ” ì •ê·œí™” ê°•ë„)
- L2 ì •ê·œí™”ëŠ” ì§ê´€ì ìœ¼ë¡œ ê°’ì´ í° ê°€ì¤‘ì¹˜ ë²¡í„°ë“¤ì—ê²Œ ì œì•½ì„ ì£¼ëŠ” ê²ƒ



```python
X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])
y = np.array([[0,1,1,0]]).T
```


```python
X
```




    array([[0, 0, 1],
           [0, 1, 1],
           [1, 0, 1],
           [1, 1, 1]])




```python
y
```




    array([[0],
           [1],
           [1],
           [0]])




```python
alpha,hidden_dim,dropout_percent,do_dropout = (0.5,4,0.2,True)
```


```python
synapse_0 = 2*np.random.random((3,hidden_dim)) - 1
synapse_1 = 2*np.random.random((hidden_dim,1)) - 1
```


```python
synapse_0
```




    array([[-0.20755129,  0.91189844,  0.99476871, -0.16371123],
           [-0.04769784,  0.95741855, -0.00352977, -0.22733522],
           [-0.01857102, -0.58750728, -0.71339113, -0.32258982]])




```python
synapse_1
```




    array([[-0.51498785],
           [-0.26549213],
           [ 0.42812075],
           [ 0.53314453]])




```python
for j in xrange(60000):
    layer_1 = (1/(1+np.exp(-(np.dot(X,synapse_0)))))
    if(do_dropout):
        layer_1 *= np.random.binomial([np.ones((len(X),hidden_dim))],1-dropout_percent)[0] * (1.0/(1-dropout_percent)) ##Dropout ë°©ë²•
    layer_2 = 1/(1+np.exp(-(np.dot(layer_1,synapse_1))))
    layer_2_delta = (layer_2 - y)*(layer_2*(1-layer_2))
    layer_1_delta = layer_2_delta.dot(synapse_1.T) * (layer_1 * (1-layer_1))
    synapse_1 -= (alpha * layer_1.T.dot(layer_2_delta))
    synapse_0 -= (alpha * X.T.dot(layer_1_delta))
```


```python
layer_1
```




    array([[  1.25000000e+00,   2.36917193e-07,   0.00000000e+00,
              7.43510583e-01],
           [  1.12459166e+00,   0.00000000e+00,   4.11308107e-08,
              5.98997274e-05],
           [  0.00000000e+00,   1.31486466e-03,   0.00000000e+00,
              0.00000000e+00],
           [  9.10115440e-28,   6.43869391e-01,   1.57284320e-04,
              9.99892234e-09]])




```python
layer_2
```




    array([[ 0.00646077],
           [ 0.88216025],
           [ 0.49747527],
           [ 0.00707326]])




```python
synapse_1
synapse_0
```




    array([[ -64.68072507,    8.62199609,    8.24917868,   -8.69799782],
           [-183.78802784,    6.9165568 ,   -9.18330184,  -10.3298174 ],
           [ 185.97856635,  -15.47928829,   -8.04635007,    0.38391467]])



<img src="http://www.kdnuggets.com/wp-content/uploads/drop-out-in-neural-networks.jpg">

### Gradient descent optimization
<img src ="http://www.aistudy.com/math/images/%EA%B5%AD%EB%B6%80%EC%A0%81%EC%B5%9C%EC%A0%81%EA%B0%92.gif">
- E(w) = loss function (output í˜•íƒœì— ë”°ë¼ ë‹¤ë¦„), sum-of-squared error, entropy error function... 
- ğ‘¤âˆ—=argminğ‘¤ğ¸(ğ‘¤) / ğ‘¤ğœ+1=ğ‘¤ğœâˆ’ğ›¼â‹…âˆ‡E(ğ‘¤ğœ) / ğ›¼ : learning rate
- Stochastic gradient descent (SGD) : SGD makes an update to the weight vector based on one data pointat a time // ğ‘¤ğœ+1=ğ‘¤ğœâˆ’ğ›¼â‹…âˆ‡ğ¸ğ‘›(ğ‘¤ğœ) 
- Error Backpropagation : ğ‘¤ğœ+1=ğ‘¤ğœâˆ’ğ›¼â‹…âˆ‡ğ¸ğ‘›(ğ‘¤ğœ)
- âˆ‡ğ¸ğ‘›(ğ‘¤ğœ) = ğœ•ğ¸ğ‘›/ğœ•ğ‘¤ğ‘—ğ‘–
<img src="https://upload.wikimedia.org/math/e/3/f/e3f0cfdaae2b1ade04cb7dcd50453b91.png">

- https://en.wikipedia.org/wiki/Delta_rule 
- https://www.quora.com/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent


```python
nn1 = Image.open('nn1.png')
nn1
```




![png](3.Review_of_Backpropagation_files/3.Review_of_Backpropagation_23_0.png)



0110ìœ¼ë¡œ í•™ìŠµí•  ê²½ìš° (x 0,1,1 y 0) learning rate 0.5


> Input D 
= I1 * Wad + I2 * Wbd + I3 * Wcd
= (0 ê³±í•˜ê¸° 0.4)+(1 ê³±í•˜ê¸° -0.6)+(1 ê³±í•˜ê¸° 0.9) =  0.3

> Input E 
= I1 * Wae + I2 * Wbe + I3 * Wce
= (0 ê³±í•˜ê¸° 0.8)+(1 ê³±í•˜ê¸° 0.2)+(1 ê³±í•˜ê¸° -0.4) = -0.2

> OutputD = 1 / (1+e-inputd) = 1 / (1+e-0.3) = 0.57
> OutputE = 1 / (1+e-inpute) = 1 / (1+e0.2) = 0.45

> InputF 
= OutputD * Wdf + OutputE * Wef
= (0.57 ê³±í•˜ê¸° -0.5) + (0.45 ê³±í•˜ê¸° 0.1) = -0.24

> OutputF 
= 1 / 1+ e-InputF) = 1 / 1 + e0.24 = 0.44

> ErrorF
= OutputF(1-OutputF)(ActualF-OutputF)
= 0.44 (1 - 0.44) (0- 0.44) = -0.108

> ErrorD
= OutputD(1-OutputD)(ErrorF * Wdf)
= 0.57(1-0.57)(-0.108 ê³±í•˜ê¸° -0.5) = 0.013

> ErrorE 
= OutputE(1-OutputE)(ErrorF * Wef)
= 0.45(1-0.45)(-0.108 ê³±í•˜ê¸° 0.1) = -0.003

> 
Wij = Wij + l * Errorj ê³±í•˜ê¸° Outputi
- Wad = 0.4 + 0.5 ê³±í•˜ê¸° 0.013 ê³±í•˜ê¸° 0 = 0.4
- Wae = 0.8 + 0.5 ê³±í•˜ê¸° -0.003 ê³±í•˜ê¸° 0 = 0.8
- Wbd = -0.6 + 0.5 ê³±í•˜ê¸° 0.013 ê³±í•˜ê¸° 1 = -0.594
...


```python
nn2 = Image.open('nn2.png')
nn2
```




![png](3.Review_of_Backpropagation_files/3.Review_of_Backpropagation_25_0.png)


